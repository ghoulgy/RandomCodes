{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "import numpy as np\n",
    "import nltk\n",
    "import sklearn\n",
    "import scipy.stats\n",
    "from sklearn.metrics import make_scorer,classification_report\n",
    "from sklearn.model_selection import cross_val_score, RandomizedSearchCV, train_test_split, cross_val_predict\n",
    "\n",
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import scorers, CRF, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('ner_dataset.csv', encoding='latin1', low_memory=False, dtype={'Sentence #': str, 'Word': str, 'POS': str, 'Tag': str})\n",
    "filled_data = data.fillna(method='ffill')\n",
    "\n",
    "class SentenceGetter(object):\n",
    "    def __init__(self, data):\n",
    "        self.n_sent = 1\n",
    "        self.data = data\n",
    "        self.empty = False\n",
    "        agg_func = lambda s: [(w, p, t) for w, p, t in zip(s[\"Word\"].values.tolist(),\n",
    "                                                           s[\"POS\"].values.tolist(),\n",
    "                                                            s[\"Tag\"].values.tolist())]\n",
    "        self.grouped = self.data.groupby(\"Sentence #\").apply(agg_func)\n",
    "        self.sentences = [s for s in self.grouped]\n",
    "  \n",
    "    def get_next(self):     \n",
    "        try:\n",
    "             s = self.grouped[\"Sentence: {}\".format(self.n_sent)]\n",
    "             self.n_sent += 1\n",
    "             return s\n",
    "        except:\n",
    "             return None       \n",
    "\n",
    "getter = SentenceGetter(filled_data)\n",
    "sentences = getter.sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'bias': 1.0, 'word.lower()': 'there', 'word[-3:]': 'ere', 'word[-2:]': 're', 'word.isupper()': False, 'word.istitle()': True, 'postag': 'EX', 'postag[:2]': 'EX', 'BOS': True, '+1:word.lower()': 'is', '+1:word.istitle()': False, '+1:word.isupper()': False, '+1:postag': 'VBZ', '+1:postag[:2]': 'VB'}, {'bias': 1.0, 'word.lower()': 'is', 'word[-3:]': 'is', 'word[-2:]': 'is', 'word.isupper()': False, 'word.istitle()': False, 'postag': 'VBZ', 'postag[:2]': 'VB', '-1:word.lower()': 'there', '-1:word.istitle()': True, '-1:word.isupper()': False, '-1:postag': 'EX', '-1:postag[:2]': 'EX', '+1:word.lower()': 'no', '+1:word.istitle()': False, '+1:word.isupper()': False, '+1:postag': 'DT', '+1:postag[:2]': 'DT'}, {'bias': 1.0, 'word.lower()': 'no', 'word[-3:]': 'no', 'word[-2:]': 'no', 'word.isupper()': False, 'word.istitle()': False, 'postag': 'DT', 'postag[:2]': 'DT', '-1:word.lower()': 'is', '-1:word.istitle()': False, '-1:word.isupper()': False, '-1:postag': 'VBZ', '-1:postag[:2]': 'VB', '+1:word.lower()': 'relationship', '+1:word.istitle()': False, '+1:word.isupper()': False, '+1:postag': 'NN', '+1:postag[:2]': 'NN'}, {'bias': 1.0, 'word.lower()': 'relationship', 'word[-3:]': 'hip', 'word[-2:]': 'ip', 'word.isupper()': False, 'word.istitle()': False, 'postag': 'NN', 'postag[:2]': 'NN', '-1:word.lower()': 'no', '-1:word.istitle()': False, '-1:word.isupper()': False, '-1:postag': 'DT', '-1:postag[:2]': 'DT', '+1:word.lower()': ',', '+1:word.istitle()': False, '+1:word.isupper()': False, '+1:postag': ',', '+1:postag[:2]': ','}, {'bias': 1.0, 'word.lower()': ',', 'word[-3:]': ',', 'word[-2:]': ',', 'word.isupper()': False, 'word.istitle()': False, 'postag': ',', 'postag[:2]': ',', '-1:word.lower()': 'relationship', '-1:word.istitle()': False, '-1:word.isupper()': False, '-1:postag': 'NN', '-1:postag[:2]': 'NN', '+1:word.lower()': 'nor', '+1:word.istitle()': False, '+1:word.isupper()': False, '+1:postag': 'CC', '+1:postag[:2]': 'CC'}, {'bias': 1.0, 'word.lower()': 'nor', 'word[-3:]': 'nor', 'word[-2:]': 'or', 'word.isupper()': False, 'word.istitle()': False, 'postag': 'CC', 'postag[:2]': 'CC', '-1:word.lower()': ',', '-1:word.istitle()': False, '-1:word.isupper()': False, '-1:postag': ',', '-1:postag[:2]': ',', '+1:word.lower()': 'has', '+1:word.istitle()': False, '+1:word.isupper()': False, '+1:postag': 'VBZ', '+1:postag[:2]': 'VB'}, {'bias': 1.0, 'word.lower()': 'has', 'word[-3:]': 'has', 'word[-2:]': 'as', 'word.isupper()': False, 'word.istitle()': False, 'postag': 'VBZ', 'postag[:2]': 'VB', '-1:word.lower()': 'nor', '-1:word.istitle()': False, '-1:word.isupper()': False, '-1:postag': 'CC', '-1:postag[:2]': 'CC', '+1:word.lower()': 'there', '+1:word.istitle()': False, '+1:word.isupper()': False, '+1:postag': 'EX', '+1:postag[:2]': 'EX'}, {'bias': 1.0, 'word.lower()': 'there', 'word[-3:]': 'ere', 'word[-2:]': 're', 'word.isupper()': False, 'word.istitle()': False, 'postag': 'EX', 'postag[:2]': 'EX', '-1:word.lower()': 'has', '-1:word.istitle()': False, '-1:word.isupper()': False, '-1:postag': 'VBZ', '-1:postag[:2]': 'VB', '+1:word.lower()': 'ever', '+1:word.istitle()': False, '+1:word.isupper()': False, '+1:postag': 'RB', '+1:postag[:2]': 'RB'}, {'bias': 1.0, 'word.lower()': 'ever', 'word[-3:]': 'ver', 'word[-2:]': 'er', 'word.isupper()': False, 'word.istitle()': False, 'postag': 'RB', 'postag[:2]': 'RB', '-1:word.lower()': 'there', '-1:word.istitle()': False, '-1:word.isupper()': False, '-1:postag': 'EX', '-1:postag[:2]': 'EX', '+1:word.lower()': 'been', '+1:word.istitle()': False, '+1:word.isupper()': False, '+1:postag': 'VBN', '+1:postag[:2]': 'VB'}, {'bias': 1.0, 'word.lower()': 'been', 'word[-3:]': 'een', 'word[-2:]': 'en', 'word.isupper()': False, 'word.istitle()': False, 'postag': 'VBN', 'postag[:2]': 'VB', '-1:word.lower()': 'ever', '-1:word.istitle()': False, '-1:word.isupper()': False, '-1:postag': 'RB', '-1:postag[:2]': 'RB', '+1:word.lower()': 'a', '+1:word.istitle()': False, '+1:word.isupper()': False, '+1:postag': 'DT', '+1:postag[:2]': 'DT'}, {'bias': 1.0, 'word.lower()': 'a', 'word[-3:]': 'a', 'word[-2:]': 'a', 'word.isupper()': False, 'word.istitle()': False, 'postag': 'DT', 'postag[:2]': 'DT', '-1:word.lower()': 'been', '-1:word.istitle()': False, '-1:word.isupper()': False, '-1:postag': 'VBN', '-1:postag[:2]': 'VB', '+1:word.lower()': 'relationship', '+1:word.istitle()': False, '+1:word.isupper()': False, '+1:postag': 'NN', '+1:postag[:2]': 'NN'}, {'bias': 1.0, 'word.lower()': 'relationship', 'word[-3:]': 'hip', 'word[-2:]': 'ip', 'word.isupper()': False, 'word.istitle()': False, 'postag': 'NN', 'postag[:2]': 'NN', '-1:word.lower()': 'a', '-1:word.istitle()': False, '-1:word.isupper()': False, '-1:postag': 'DT', '-1:postag[:2]': 'DT', '+1:word.lower()': ',', '+1:word.istitle()': False, '+1:word.isupper()': False, '+1:postag': ',', '+1:postag[:2]': ','}, {'bias': 1.0, 'word.lower()': ',', 'word[-3:]': ',', 'word[-2:]': ',', 'word.isupper()': False, 'word.istitle()': False, 'postag': ',', 'postag[:2]': ',', '-1:word.lower()': 'relationship', '-1:word.istitle()': False, '-1:word.isupper()': False, '-1:postag': 'NN', '-1:postag[:2]': 'NN', '+1:word.lower()': 'between', '+1:word.istitle()': False, '+1:word.isupper()': False, '+1:postag': 'IN', '+1:postag[:2]': 'IN'}, {'bias': 1.0, 'word.lower()': 'between', 'word[-3:]': 'een', 'word[-2:]': 'en', 'word.isupper()': False, 'word.istitle()': False, 'postag': 'IN', 'postag[:2]': 'IN', '-1:word.lower()': ',', '-1:word.istitle()': False, '-1:word.isupper()': False, '-1:postag': ',', '-1:postag[:2]': ',', '+1:word.lower()': 'our', '+1:word.istitle()': False, '+1:word.isupper()': False, '+1:postag': 'PRP$', '+1:postag[:2]': 'PR'}, {'bias': 1.0, 'word.lower()': 'our', 'word[-3:]': 'our', 'word[-2:]': 'ur', 'word.isupper()': False, 'word.istitle()': False, 'postag': 'PRP$', 'postag[:2]': 'PR', '-1:word.lower()': 'between', '-1:word.istitle()': False, '-1:word.isupper()': False, '-1:postag': 'IN', '-1:postag[:2]': 'IN', '+1:word.lower()': 'mustard', '+1:word.istitle()': False, '+1:word.isupper()': False, '+1:postag': 'NN', '+1:postag[:2]': 'NN'}, {'bias': 1.0, 'word.lower()': 'mustard', 'word[-3:]': 'ard', 'word[-2:]': 'rd', 'word.isupper()': False, 'word.istitle()': False, 'postag': 'NN', 'postag[:2]': 'NN', '-1:word.lower()': 'our', '-1:word.istitle()': False, '-1:word.isupper()': False, '-1:postag': 'PRP$', '-1:postag[:2]': 'PR', '+1:word.lower()': 'and', '+1:word.istitle()': False, '+1:word.isupper()': False, '+1:postag': 'CC', '+1:postag[:2]': 'CC'}, {'bias': 1.0, 'word.lower()': 'and', 'word[-3:]': 'and', 'word[-2:]': 'nd', 'word.isupper()': False, 'word.istitle()': False, 'postag': 'CC', 'postag[:2]': 'CC', '-1:word.lower()': 'mustard', '-1:word.istitle()': False, '-1:word.isupper()': False, '-1:postag': 'NN', '-1:postag[:2]': 'NN', '+1:word.lower()': 'the', '+1:word.istitle()': False, '+1:word.isupper()': False, '+1:postag': 'DT', '+1:postag[:2]': 'DT'}, {'bias': 1.0, 'word.lower()': 'the', 'word[-3:]': 'the', 'word[-2:]': 'he', 'word.isupper()': False, 'word.istitle()': False, 'postag': 'DT', 'postag[:2]': 'DT', '-1:word.lower()': 'and', '-1:word.istitle()': False, '-1:word.isupper()': False, '-1:postag': 'CC', '-1:postag[:2]': 'CC', '+1:word.lower()': 'country', '+1:word.istitle()': False, '+1:word.isupper()': False, '+1:postag': 'NN', '+1:postag[:2]': 'NN'}, {'bias': 1.0, 'word.lower()': 'country', 'word[-3:]': 'try', 'word[-2:]': 'ry', 'word.isupper()': False, 'word.istitle()': False, 'postag': 'NN', 'postag[:2]': 'NN', '-1:word.lower()': 'the', '-1:word.istitle()': False, '-1:word.isupper()': False, '-1:postag': 'DT', '-1:postag[:2]': 'DT', '+1:word.lower()': 'of', '+1:word.istitle()': False, '+1:word.isupper()': False, '+1:postag': 'IN', '+1:postag[:2]': 'IN'}, {'bias': 1.0, 'word.lower()': 'of', 'word[-3:]': 'of', 'word[-2:]': 'of', 'word.isupper()': False, 'word.istitle()': False, 'postag': 'IN', 'postag[:2]': 'IN', '-1:word.lower()': 'country', '-1:word.istitle()': False, '-1:word.isupper()': False, '-1:postag': 'NN', '-1:postag[:2]': 'NN', '+1:word.lower()': 'france', '+1:word.istitle()': True, '+1:word.isupper()': False, '+1:postag': 'NNP', '+1:postag[:2]': 'NN'}, {'bias': 1.0, 'word.lower()': 'france', 'word[-3:]': 'nce', 'word[-2:]': 'ce', 'word.isupper()': False, 'word.istitle()': True, 'postag': 'NNP', 'postag[:2]': 'NN', '-1:word.lower()': 'of', '-1:word.istitle()': False, '-1:word.isupper()': False, '-1:postag': 'IN', '-1:postag[:2]': 'IN', '+1:word.lower()': '.', '+1:word.istitle()': False, '+1:word.isupper()': False, '+1:postag': '.', '+1:postag[:2]': '.'}, {'bias': 1.0, 'word.lower()': '.', 'word[-3:]': '.', 'word[-2:]': '.', 'word.isupper()': False, 'word.istitle()': False, 'postag': '.', 'postag[:2]': '.', '-1:word.lower()': 'france', '-1:word.istitle()': True, '-1:word.isupper()': False, '-1:postag': 'NNP', '-1:postag[:2]': 'NN', 'EOS': True}]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Wall time: 2.42 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# train_sents = list(nltk.corpus.conll2002.iob_sents('esp.train'))\n",
    "# test_sents = list(nltk.corpus.conll2002.iob_sents('esp.testb'))\n",
    "\n",
    "def word2features(sent, i):\n",
    "    word = sent[i][0]\n",
    "    postag = sent[i][1]\n",
    "\n",
    "    features = {\n",
    "        'bias': 1.0,\n",
    "        'word.lower()': word.lower(),\n",
    "        'word[-3:]': word[-3:],\n",
    "        'word[-2:]': word[-2:],\n",
    "        'word.isupper()': word.isupper(),\n",
    "        'word.istitle()': word.istitle(),\n",
    "        'postag': postag,\n",
    "        'postag[:2]': postag[:2],\n",
    "    }\n",
    "    if i > 0:\n",
    "        word1 = sent[i-1][0]\n",
    "        postag1 = sent[i-1][1]\n",
    "        features.update({\n",
    "            '-1:word.lower()': word1.lower(),\n",
    "            '-1:word.istitle()': word1.istitle(),\n",
    "            '-1:word.isupper()': word1.isupper(),\n",
    "            '-1:postag': postag1,\n",
    "            '-1:postag[:2]': postag1[:2],\n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True\n",
    "\n",
    "    if i < len(sent)-1:\n",
    "        word1 = sent[i+1][0]\n",
    "        postag1 = sent[i+1][1]\n",
    "        features.update({\n",
    "            '+1:word.lower()': word1.lower(),\n",
    "            '+1:word.istitle()': word1.istitle(),\n",
    "            '+1:word.isupper()': word1.isupper(),\n",
    "            '+1:postag': postag1,\n",
    "            '+1:postag[:2]': postag1[:2],\n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return [label for token, postag, label in sent]\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    return [token for token, postag, label in sent]\n",
    "\n",
    "# X = [sent2features(s) for s in train_sents]\n",
    "# y = [sent2labels(s) for s in train_sents]\n",
    "\n",
    "# X_test = [sent2features(s) for s in test_sents]\n",
    "# y_test = [sent2labels(s) for s in test_sents]\n",
    "\n",
    "X = [sent2features(s) for s in sentences]\n",
    "y = [sent2labels(s) for s in sentences]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "print(X_test[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "crf = CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.1,\n",
    "    c2=0.1,\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=False\n",
    ")\n",
    "crf.fit(X_train, y_train)\n",
    "\n",
    "# pred = cross_val_predict(estimator=crf, X=X, y=y, cv=5, n_jobs=-1, verbose=1)\n",
    "# report = classification_report(y_pred=pred, y_true=y)\n",
    "# print(report)\n",
    "\n",
    "labels = list(crf.classes_)\n",
    "labels.remove('O')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B-PER', 'I-PER', 'B-TIM', 'B-MAL', 'B-ORG', 'I-ORG', 'B-GPE', 'I-MAL', 'I-TIM', 'B-NAT', 'I-GPE', 'B-EVE', 'B-ART', 'I-EVE', 'I-ART', 'I-NAT']\n"
     ]
    }
   ],
   "source": [
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# define fixed parameters and parameters to search\n",
    "crf = CRF(\n",
    "    algorithm='lbfgs',\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=False\n",
    ")\n",
    "params_space = {\n",
    "    'c1': scipy.stats.expon(scale=0.5),\n",
    "    'c2': scipy.stats.expon(scale=0.05),\n",
    "}\n",
    "\n",
    "# use the same metric for evaluation\n",
    "f1_scorer = make_scorer(metrics.flat_f1_score,\n",
    "                        average='weighted', labels=labels)\n",
    "\n",
    "# search\n",
    "rs = RandomizedSearchCV(crf, params_space,\n",
    "                        cv=3,\n",
    "                        verbose=4,\n",
    "                        n_jobs=-1,\n",
    "                        n_iter=50,\n",
    "                        scoring=f1_scorer)\n",
    "rs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = 'rs_finalized_model.sav'\n",
    "pickle.dump(rs, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('best params:', rs.best_params_)\n",
    "print('best cv score:', rs.best_score_)\n",
    "print('model size {:0.2f}M'.format(rs.best_estimator_.size_ / 1000000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crf = rs.best_estimator_\n",
    "y_pred = crf.predict(X_test)\n",
    "\n",
    "sorted_labels = sorted(\n",
    "    labels,\n",
    "    key=lambda name: (name[1:], name[0])\n",
    ")\n",
    "\n",
    "print(metrics.flat_classification_report(\n",
    "    y_test, y_pred, labels=sorted_labels, digits=3\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def print_transitions(trans_features):\n",
    "    for (label_from, label_to), weight in trans_features:\n",
    "        print(\"%-6s -> %-7s %0.6f\"%(label_from, label_to, weight))\n",
    "\n",
    "print(\"Top likely transitions:\")\n",
    "print_transitions(Counter(crf.transition_features_).most_common(20))\n",
    "\n",
    "print(\"\\nTop unlikely transitions:\")\n",
    "print_transitions(Counter(crf.transition_features_).most_common()[-20:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import eli5\n",
    "\n",
    "eli5.show_weights(crf, top=30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
