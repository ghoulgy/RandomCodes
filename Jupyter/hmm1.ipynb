{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from collections import defaultdict, Counter\n",
    "import scipy.stats\n",
    "from sklearn.metrics import make_scorer,classification_report\n",
    "from sklearn.model_selection import cross_val_score, RandomizedSearchCV, train_test_split, cross_val_predict\n",
    "from pomegranate import State, HiddenMarkovModel, DiscreteDistribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3248 812 3248 812\n",
      "Wall time: 4.44 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('final_dataset.csv', encoding='latin1', low_memory=False, dtype={'Sentence #': str, 'Word': str, 'POS': str, 'Tag': str})\n",
    "filled_data = data.fillna(method='ffill')\n",
    "\n",
    "class SentenceGetter(object):\n",
    "    def __init__(self, data):\n",
    "        self.n_sent = 1\n",
    "        self.data = data\n",
    "        self.empty = False\n",
    "        agg_func = lambda s: [(w, t) for w, t in zip(s[\"Word\"].values.tolist(),\n",
    "                                                    s[\"Tag\"].values.tolist())]\n",
    "        self.grouped = self.data.groupby(\"Sentence #\").apply(agg_func)\n",
    "        self.sentences = [s for s in self.grouped]\n",
    "        self.length = int(len(self.sentences))\n",
    "        \n",
    "        # All Words group by sentence\n",
    "        agg_func_word = lambda s: [w for w in s[\"Word\"].values.tolist()]             \n",
    "        self.grouped_word = self.data.groupby(\"Sentence #\").apply(agg_func_word)\n",
    "        self.sentences_words = [s for s in self.grouped_word]\n",
    "        \n",
    "        # All Tags group by sentence\n",
    "        agg_func_tag = lambda s: [t for t in s[\"Tag\"].values.tolist()]\n",
    "        self.grouped_tag = self.data.groupby(\"Sentence #\").apply(agg_func_tag)\n",
    "        self.sentences_tags = [s for s in self.grouped_tag]\n",
    "        \n",
    "        # 80% Train 20% Test\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(self.sentences_words, self.sentences_tags, test_size=0.2)\n",
    "        \n",
    "\n",
    "    def get_next(self):     \n",
    "        try:\n",
    "             s = self.grouped[\"Sentence: {}\".format(self.n_sent)]\n",
    "             self.n_sent += 1\n",
    "             return s\n",
    "        except:\n",
    "             return None       \n",
    "\n",
    "getter = SentenceGetter(filled_data)\n",
    "sentences = getter.sentences\n",
    "words = getter.sentences_words\n",
    "tags = getter.sentences_tags\n",
    "\n",
    "# X = Words, y = Tags\n",
    "X_train = getter.X_train\n",
    "X_test = getter.X_test\n",
    "y_train = getter.y_train\n",
    "y_test = getter.y_test\n",
    "\n",
    "y_train_stream = []\n",
    "y_test_stream = []\n",
    "for train_st, test_st in zip(y_train, y_test):\n",
    "    for train_t, test_t in zip(train_st, test_st):\n",
    "        y_train_stream.append(train_t)\n",
    "        y_test_stream.append(test_t)\n",
    "\n",
    "X_train_stream = []\n",
    "X_test_stream = []\n",
    "for train_st, test_st in zip(X_train, X_test):\n",
    "    for train_t, test_t in zip(train_st, test_st):\n",
    "        X_train_stream.append(train_t)\n",
    "        X_test_stream.append(test_t)\n",
    "\n",
    "train_vocab = frozenset(X_train_stream)\n",
    "print(len(X_train), len(X_test), len(y_train), len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pair_counts(tags, words):\n",
    "    d = defaultdict(lambda: defaultdict(int))\n",
    "    for tag, word in zip(tags, words):\n",
    "        d[tag][word] += 1\n",
    "    \n",
    "    return d\n",
    "\n",
    "\n",
    "all_single_words = [] # X\n",
    "all_single_tags = [] # Y\n",
    "for sentence in sentences:\n",
    "    for i, (word, tag) in enumerate(sentence):\n",
    "        all_single_words.append(word)\n",
    "        all_single_tags.append(tag)\n",
    "\n",
    "data_wt = [*zip(all_single_words, all_single_tags)] # data_wt = data.stream()\n",
    "\n",
    "# print(words[0])\n",
    "# X_train, X_test, y_train, y_test = train_test_split(words, tags, test_size=0.2)\n",
    "# print(len(X_train), len(X_test), len(y_train), len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unigram_counts(seqs):\n",
    "    return Counter(seqs)\n",
    "\n",
    "def bigram_counts(seqs):\n",
    "    d = Counter(seqs)\n",
    "    return d\n",
    "\n",
    "# tag_uni = [tag for i, (w, t) in enumerate(train_set)]\n",
    "tag_unigrams = unigram_counts(y_train_stream)\n",
    "\n",
    "tag_bi = [t for i, (w, t) in enumerate(data_wt)]\n",
    "bi = [(tag_bi[i], tag_bi[i+1]) for i in range(0, len(tag_bi)-2, 2)]\n",
    "tag_bigrams = bigram_counts(bi)\n",
    "# print(tag_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'O': 3850, 'B-MAL': 154, 'B-CAP': 20, 'B-BEH': 18, 'B-DVEC': 16, 'B-OS': 2})\n",
      "Counter({'O': 4054, 'B-MAL': 6})\n"
     ]
    }
   ],
   "source": [
    "def starting_counts(seqs):\n",
    "    d = Counter(seqs)\n",
    "    return d\n",
    "\n",
    "def ending_counts(seqs):\n",
    "    d = Counter(seqs)\n",
    "    return d\n",
    "\n",
    "starting_tag_list = [i[0] for i in tags]\n",
    "tag_starts = starting_counts(starting_tag_list)\n",
    "ending_tag_list = [i[-1] for i in tags]\n",
    "tag_ends = ending_counts(ending_tag_list)\n",
    "print(tag_starts)\n",
    "print(tag_ends)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'O': 0.038570970585878016, 'B-OS': 0.010752688172043012, 'B-DVEC': 0.043010752688172046, 'I-DVEC': 0.0, 'B-CAP': 0.062111801242236024, 'B-MAL': 0.11079136690647481, 'B-BEH': 0.021634615384615384, 'I-BEH': 0.0, 'I-CAP': 0.0, 'I-MAL': 0.0, 'I-OS': 0.0}\n",
      "{'O': 0.04061473110523363, 'B-OS': 0.0, 'B-DVEC': 0.0, 'I-DVEC': 0.0, 'B-CAP': 0.0, 'B-MAL': 0.004316546762589928, 'B-BEH': 0.0, 'I-BEH': 0.0, 'I-CAP': 0.0, 'I-MAL': 0.0, 'I-OS': 0.0}\n"
     ]
    }
   ],
   "source": [
    "hmm = HiddenMarkovModel(name=\"base-hmm-tagger\")\n",
    "\n",
    "all_single_words = [w for i, (w, t) in enumerate(data_wt)]\n",
    "all_single_tags = [t for i, (w, t) in enumerate(data_wt)]\n",
    "\n",
    "tags_count = unigram_counts(all_single_tags)\n",
    "tag_words_count = pair_counts(all_single_tags, all_single_words)\n",
    "\n",
    "starting_tag_count = starting_counts(starting_tag_list)\n",
    "ending_tag_count = ending_counts(ending_tag_list)\n",
    "\n",
    "to_pass_states = []\n",
    "\n",
    "for tag, words_dict in tag_words_count.items():\n",
    "    total = float(sum(words_dict.values()))\n",
    "    distribution = {word: count/total for word, count in words_dict.items()}\n",
    "    tag_emissions = DiscreteDistribution(distribution)\n",
    "    tag_state = State(tag_emissions, name=tag)\n",
    "    to_pass_states.append(tag_state)\n",
    "\n",
    "hmm.add_states()\n",
    "\n",
    "start_prob={}\n",
    "\n",
    "for tag in all_single_tags:\n",
    "    start_prob[tag] = starting_tag_count[tag]/tags_count[tag]\n",
    "print(start_prob)\n",
    "for tag_state in to_pass_states:\n",
    "    hmm.add_transition(hmm.start, tag_state, start_prob[tag_state.name])\n",
    "\n",
    "end_prob = {}\n",
    "\n",
    "for tag in all_single_tags:\n",
    "    end_prob[tag] = ending_tag_count[tag]/tags_count[tag]\n",
    "print(end_prob)\n",
    "for tag_state in to_pass_states:\n",
    "    hmm.add_transition(tag_state, hmm.end, end_prob[tag_state.name])\n",
    "    \n",
    "transition_prob_pair = {}\n",
    "\n",
    "for key in tag_bigrams.keys():\n",
    "    transition_prob_pair[key] = tag_bigrams.get(key)/tags_count[key[0]]\n",
    "\n",
    "for tag_state in to_pass_states:\n",
    "    for next_tag_state in to_pass_states:\n",
    "        if (tag_state.name, next_tag_state.name) in tag_bigrams.keys():\n",
    "#             print(*zip([tag_state.name], [next_tag_state.name]))\n",
    "            hmm.add_transition(tag_state,next_tag_state,transition_prob_pair[(tag_state.name,next_tag_state.name)])\n",
    "\n",
    "hmm.bake()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_unknown(sequence):\n",
    "    \n",
    "    return [w if w in train_vocab else 'nan' for w in sequence]\n",
    "\n",
    "def simplify_decoding(X, model):\n",
    "    \n",
    "    _, state_path = model.viterbi(replace_unknown(X))\n",
    "    return [state[1].name for state in state_path[1:-1]]\n",
    "\n",
    "def accuracy(X, Y, model):\n",
    "    \n",
    "    correct = total_predictions = 0\n",
    "    for observations, actual_tags in zip(X, Y):\n",
    "        \n",
    "        # The model.viterbi call in simplify_decoding will return None if the HMM\n",
    "        # raises an error (for example, if a test sentence contains a word that\n",
    "        # is out of vocabulary for the training set). Any exception counts the\n",
    "        # full sentence as an error (which makes this a conservative estimate).\n",
    "        try:\n",
    "            most_likely_tags = simplify_decoding(observations, model)\n",
    "            correct += sum(p == t for p, t in zip(most_likely_tags, actual_tags))\n",
    "        except:\n",
    "            pass\n",
    "        total_predictions += len(observations)\n",
    "    return correct / total_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy basic hmm model: 97.61%\n",
      "testing accuracy basic hmm model: 97.47%\n"
     ]
    }
   ],
   "source": [
    "hmm_training_acc = accuracy(X_train, y_train, hmm)\n",
    "print(\"training accuracy basic hmm model: {:.2f}%\".format(100 * hmm_training_acc))\n",
    "\n",
    "hmm_testing_acc = accuracy(X_test, y_test, hmm)\n",
    "print(\"testing accuracy basic hmm model: {:.2f}%\".format(100 * hmm_testing_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted labels:\n",
      "-----------------\n",
      "['O', 'O', 'O', 'B-OS', 'O', 'O', 'B-DVEC', 'I-DVEC', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Actual labels:\n",
      "--------------\n",
      "['O', 'O', 'O', 'B-OS', 'O', 'O', 'B-DVEC', 'I-DVEC', 'O', 'O', 'B-CAP', 'O']\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Predicted labels:\\n-----------------\")\n",
    "print(simplify_decoding(words[2], hmm))\n",
    "print()\n",
    "print(\"Actual labels:\\n--------------\")\n",
    "print(tags[2])\n",
    "print()\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
